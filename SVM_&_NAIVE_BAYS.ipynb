{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1:  What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes.\n",
        "\n",
        " Core Concepts of SVM\n",
        "\n",
        "• \tHyperplane: This is the decision boundary that separates different classes. In 2D, it’s a line; in 3D, it’s a plane; in higher dimensions, it’s still called a hyperplane.\n",
        "\n",
        "• \tSupport Vectors: These are the data points closest to the hyperplane. They are critical in defining the position and orientation of the hyperplane.\n",
        "\n",
        "• \tMargin: The distance between the hyperplane and the nearest support vectors. SVM aims to maximize this margin to improve generalization on unseen data.\n",
        "\n",
        "\n",
        " How SVM Works\n",
        "\n",
        "1. \tLinear Separation:\n",
        "\n",
        "• \tFor linearly separable data, SVM finds the hyperplane that maximizes the margin between classes.\n",
        "\n",
        "• \tThe optimal hyperplane is defined by the equation w \\cdot x + b = 0, where w is the weight vector and b is the bias.\n",
        "\n",
        "2. \tNon-linear Separation:\n",
        "\n",
        "• \tReal-world data is often not linearly separable.\n",
        "\n",
        "• \tSVM uses kernel functions (like polynomial, radial basis function) to transform data into a higher-dimensional space where a linear separator can be found.\n",
        "\n",
        "3. \tSoft Margin vs. Hard Margin:\n",
        "\n",
        "• \tHard Margin: Assumes perfect separation with no misclassifications.\n",
        "\n",
        "• \tSoft Margin: Allows some misclassifications to improve robustness and generalization, especially in noisy datasets.\n",
        "\n",
        "4. \tOptimization:\n",
        "\n",
        "• \tSVM solves a quadratic optimization problem to find the best hyperplane.\n",
        "\n",
        "• \tAlgorithms like Sequential Minimal Optimization (SMO) are used for efficient computation.\n",
        "\n",
        "\n",
        " Why Use SVM?\n",
        "\n",
        "• \tEffective in high-dimensional spaces.\n",
        "\n",
        "• \tRobust to outliers, especially with soft margin.\n",
        "\n",
        "• \tWorks well for both linear and non-linear classification.\n",
        "\n",
        "• \tStrong theoretical foundation from statistical learning theory."
      ],
      "metadata": {
        "id": "9s3O2IPDX-rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        " - The difference between Hard Margin and Soft Margin SVM lies in how strictly the algorithm separates the data and handles misclassifications.\n",
        "\n",
        " Hard Margin SVM\n",
        "\n",
        "• \tAssumes perfect separation: It tries to find a hyperplane that separates the classes without any errors.\n",
        "\n",
        "• \tNo tolerance for misclassification: Every data point must be correctly classified and lie outside the margin.\n",
        "\n",
        "• \tWorks only when data is linearly separable.\n",
        "\n",
        "• \tHighly sensitive to outliers: A single misclassified point can make it impossible to find a valid hyperplane.\n",
        "\n",
        "\n",
        " Soft Margin SVM\n",
        "\n",
        "• \tAllows some misclassifications: Introduces a slack variable to permit violations of the margin.\n",
        "\n",
        "• \tBalances margin maximization with classification error: Uses a regularization parameter C to control the trade-off:\n",
        "\n",
        "• \tHigh C: Less tolerance for errors (closer to hard margin).\n",
        "\n",
        "• \tLow C: More tolerance for errors (wider margin).\n",
        "\n",
        "• \tHandles noisy and overlapping data better.\n",
        "\n",
        "• \tMore practical for real-world datasets that aren’t perfectly separable.\n",
        "\n",
        "• \tMisclassification:\n",
        "\n",
        "• \tHard Margin: Not allowed.\n",
        "\n",
        "• \tSoft Margin: Allowed, controlled by a regularization parameter C.\n",
        "\n",
        "• \tData Requirement:\n",
        "\n",
        "• \tHard Margin: Requires perfectly separable data.\n",
        "\n",
        "• \tSoft Margin: Can handle overlapping or noisy data.\n",
        "\n",
        "• \tRobustness to Noise:\n",
        "\n",
        "• \tHard Margin: Low — sensitive to outliers.\n",
        "\n",
        "• \tSoft Margin: High — more tolerant of noise.\n",
        "\n",
        "• \tUse Case:\n",
        "\n",
        "• \tHard Margin: Idealized, clean datasets.\n",
        "\n",
        "• \tSoft Margin: Practical, real-world applications."
      ],
      "metadata": {
        "id": "sZnE--UOYqq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "- The Kernel Trick is a powerful technique used in Support Vector Machines (SVMs) to handle non-linearly separable data by implicitly mapping it into a higher-dimensional space—without ever computing the transformation explicitly.\n",
        "\n",
        " What Is the Kernel Trick?\n",
        "\n",
        "• \tInstead of transforming data points x into a higher-dimensional space \\phi(x), the kernel trick computes the dot product K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j) directly.\n",
        "\n",
        "• \tThis allows SVM to find a linear separator in the transformed space, which corresponds to a non-linear boundary in the original space.\n",
        "\n",
        "• \tIt’s computationally efficient and avoids the curse of dimensionality.\n",
        "\n",
        " Example: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "• \tFormula:\n",
        "\n",
        "K(x_i, x_j) = \\exp\\left(-\\gamma \\|x_i - x_j\\|^2\\right)\n",
        "where \\gamma is a parameter that controls the influence of a single training example.\n",
        "• \tUse Case:\n",
        "\n",
        "Ideal for complex, non-linear classification problems where the decision boundary is not a straight line.\n",
        "\n",
        "For example:\n",
        "\n",
        "• \tHandwritten digit recognition (like MNIST)\n",
        "\n",
        "• \tBioinformatics (e.g., protein classification)\n",
        "\n",
        "• \tImage classification with overlapping features\n",
        "\n",
        "• \tWhy It Works:\n",
        "\n",
        "The RBF kernel creates a localized influence around each data point, allowing the SVM to build flexible decision boundaries that adapt to the data’s shape."
      ],
      "metadata": {
        "id": "uzfWYCZlZfa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "- The Naïve Bayes Classifier is a simple yet powerful probabilistic machine learning algorithm used for classification tasks. It’s based on Bayes’ Theorem, which describes the probability of a class given some features.\n",
        "\n",
        " How It WorkS\n",
        "\n",
        "Naïve Bayes calculates the posterior probability of a class C given a feature vector X = (x_1, x_2, ..., x_n) using:\n",
        "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
        "\n",
        "Where:\n",
        "\n",
        "• \tP(C|X) is the probability of class C given features X\n",
        "\n",
        "• \tP(X|C) is the likelihood of features given class\n",
        "\n",
        "• \tP(C) is the prior probability of class\n",
        "\n",
        "• \tP(X) is the evidence (often ignored in classification since it’s constant across classes)\n",
        "\n",
        " Why Is It Called “Naïve”?\n",
        "\n",
        "It’s called naïve because it makes a strong assumption:\n",
        "\n",
        "\n",
        "This assumption is rarely true in real-world data (features often correlate), but the algorithm still performs surprisingly well in many applications.\n",
        "\n",
        " Use Cases\n",
        "\n",
        "• \tText classification (e.g., spam detection, sentiment analysis)\n",
        "\n",
        "• \tMedical diagnosis\n",
        "\n",
        "• \tDocument categorization\n",
        "\n",
        "• \tRecommendation systems\n",
        "\n",
        "\n",
        " Strengths\n",
        "\n",
        "• \tFast and efficient, even with large datasets\n",
        "\n",
        "• \tPerforms well with high-dimensional data\n",
        "\n",
        "• \tRequires relatively little training data"
      ],
      "metadata": {
        "id": "JP1Lc6QbZxAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "- The Naïve Bayes classifier has several variants tailored to different types of data. The three most common are Gaussian, Multinomial, and Bernoulli Naïve Bayes. Here's how they differ and when you'd use each:\n",
        "\n",
        " 1. Gaussian Naïve BayeS\n",
        "\n",
        "• \tAssumes: Features follow a normal (Gaussian) distribution.\n",
        "\n",
        "• \tUse Case: When your features are continuous numerical values.\n",
        "\n",
        "• \tExample: Predicting whether a patient has a disease based on continuous features like blood pressure, cholesterol level, or age.\n",
        "\n",
        "Why use it?\n",
        "\n",
        "It models the likelihood of features using the Gaussian probability density function, making it ideal for real-valued inputs.\n",
        "\n",
        "\n",
        " 2. Multinomial Naïve Bayes\n",
        "\n",
        "• \tAssumes: Features represent discrete counts (e.g., word frequencies).\n",
        "\n",
        "• \tUse Case: Best for text classification problems like spam detection, sentiment analysis, or document categorization.\n",
        "\n",
        "• \tExample: Classifying emails based on word occurrence counts.\n",
        "\n",
        "Why use it?\n",
        "\n",
        "It’s designed to handle data where features are counts or frequencies, making it perfect for bag-of-words models in NLP.\n",
        "\n",
        " 3. Bernoulli Naïve Bayes\n",
        "\n",
        "• \tAssumes: Features are binary (0 or 1), indicating presence or absence.\n",
        "\n",
        "• \tUse Case: Also used in text classification, but focuses on word appears in a document -not how aften.\n",
        "\n",
        "• \tExample: Determining if a tweet is positive or negative based on presence of specific keywords.\n",
        "\n",
        "Why use it?\n",
        "\n",
        "It’s useful when your features are binary indicators, especially in sparse datasets.\n",
        "\n",
        " Gaussian, Multinomial, and Bernoulli Naïve Bayes variants written out line-by-line:\n",
        "\n",
        "• \tGaussian Naïve Bayes\n",
        "\n",
        "• \tAssumes features follow a normal (Gaussian) distribution.\n",
        "\n",
        "• \tBest for continuous numerical data.\n",
        "\n",
        "• \tCommonly used in medical diagnosis or sensor-based predictions.\n",
        "\n",
        "• \tMultinomial Naïve Bayes\n",
        "\n",
        "• \tAssumes features are discrete counts (e.g., word frequencies).\n",
        "\n",
        "• \tBest for text classification tasks like spam detection or document categorization.\n",
        "\n",
        "• \tIdeal when using bag-of-words or term frequency representations.\n",
        "\n",
        "• \tBernoulli Naïve Bayes\n",
        "\n",
        "• \tAssumes binary features (presence or absence).\n",
        "\n",
        "• \tBest for text classification where features indicate whether a word appears or not.\n",
        "\n",
        "• \tSuitable for sparse binary datasets."
      ],
      "metadata": {
        "id": "Bz3MvyqUaWSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6:   Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "- Python program that loads the Iris dataset, trains an SVM classifier with a linear kernel, and prints the model's accuracy and support vectors:"
      ],
      "metadata": {
        "id": "EWOd552mcZsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R55Iyok4coPk",
        "outputId": "86e1dc09-2491-49d0-fe6b-0624860005c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7:  Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "- Python program that loads the Breast Cancer dataset, trains a Gaussian Naïve Bayes classifier, and prints the classification report including precision, recall, and F1-score:"
      ],
      "metadata": {
        "id": "DgliYwy6cwZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3QYroe-dbUx",
        "outputId": "9614d80e-caf7-4fe1-f2d3-e9d28531dbb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "- Python program that trains an SVM classifier on the Wine dataset using  to find the best values for  and , then prints the best hyperparameters and accuracy:"
      ],
      "metadata": {
        "id": "xMDGKc4CdhHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']  # Using RBF kernel for non-linear separation\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iIgA9Sodsv3",
        "outputId": "7996cbf2-f063-42b1-ab27-495e0aa8c8cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Model Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "- Python program that trains a Multinomial Naïve Bayes classifier on the  text dataset and prints the ROC-AUC score for its predictions:"
      ],
      "metadata": {
        "id": "0pSvbx64d2R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load the 20 Newsgroups dataset (binary classification for simplicity)\n",
        "categories = ['rec.sport.hockey', 'sci.space']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Binarize the labels for ROC-AUC computation\n",
        "y_bin = label_binarize(y, classes=[0, 1])\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Print results\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kac7jG2ueHky",
        "outputId": "2b0e00a9-9b11-4726-ace7-af7be612c58f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "- complete approach to building a spam classifier for email communications, along with Python code to demonstrate the pipeline:\n",
        "\n",
        " Preprocessing the Data\n",
        "\n",
        "1. \tText Vectorization:\n",
        "\n",
        "• \tUse  to convert email text into numerical features.\n",
        "\n",
        "• \tHelps normalize word frequency and reduce the impact of common words.\n",
        "\n",
        "2. \tHandling Missing Data:\n",
        "\n",
        "• \tFill missing email bodies with empty strings or use imputation if metadata is available.\n",
        "\n",
        "• \tDrop rows with critical missing labels.\n",
        "\n",
        " Model Choice: Naïve Bayes vs. SVM\n",
        "\n",
        "• \tNaïve Bayes is preferred for text classification because:\n",
        "\n",
        "• \tIt’s fast and efficient with high-dimensional sparse data.\n",
        "\n",
        "• \tAssumes feature independence, which works surprisingly well with text.\n",
        "\n",
        "• \tSVM can be powerful but is slower and less scalable for large text corpora.\n",
        "\n",
        " Chosen Model: Multinomial Naïve Bayes\n",
        "\n",
        "\n",
        " Addressing Class Imbalance\n",
        "\n",
        "• \tUse  (for SVM) or resampling techniques (e.g., SMOTE or undersampling).\n",
        "\n",
        "• \tAlternatively, adjust decision thresholds or use custom loss functions.\n",
        "\n",
        "\n",
        " Evaluation Metrics\n",
        "\n",
        "• \tPrecision: Important to avoid flagging legitimate emails as spam.\n",
        "\n",
        "• \tRecall: Important to catch as many spam emails as possible.\n",
        "\n",
        "• \tF1-score: Balances precision and recall.\n",
        "\n",
        "• \tROC-AUC: Measures overall model discrimination.\n",
        "\n",
        "\n",
        " Business Impact\n",
        "\n",
        "• \tReduces manual filtering and improves user experience.\n",
        "\n",
        "• \tProtects users from phishing and malware.\n",
        "\n",
        "• \tSaves time and resources for customer support and IT teams."
      ],
      "metadata": {
        "id": "_ODCRREUeV4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Simulate spam vs. not spam using two categories\n",
        "categories = ['talk.politics.misc', 'rec.autos']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Handle missing data\n",
        "texts = [doc if doc else \"\" for doc in data.data]\n",
        "\n",
        "# Vectorize text\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = data.target\n",
        "\n",
        "# Binarize labels for ROC-AUC\n",
        "y_bin = label_binarize(y, classes=[0, 1])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Not Spam\", \"Spam\"]))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAuvMP-DfC2j",
        "outputId": "06db3c55-e3d1-44f0-eea1-9e69c9a920e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Not Spam       0.92      0.97      0.94       300\n",
            "        Spam       0.96      0.89      0.92       230\n",
            "\n",
            "    accuracy                           0.93       530\n",
            "   macro avg       0.94      0.93      0.93       530\n",
            "weighted avg       0.94      0.93      0.93       530\n",
            "\n",
            "ROC-AUC Score: 0.9846811594202899\n"
          ]
        }
      ]
    }
  ]
}